# ResMLP 论文核心实验内容总结

本文档总结了关于 `ResMLP` 架构的系列论文实验，旨在全面理解其设计、性能和工作机理。

---

## 实验一：基准性能对比 (Benchmark Comparison)

**目的：** 在一个公平的标准化环境（ImageNet-1k 分类任务）下，将新提出的 `ResMLP` 模型与当前主流的卷积网络 (CNNs) 和视觉 Transformer (ViTs) 进行全面的性能与效率对比。

**方法：**

* **参赛模型：**
  * **State of the art:** CaiT, NFNet (性能标杆)
  * **卷积网络:** EfficientNet, RegNetY (经典架构)
  * **Transformer 网络:** DeiT, CaiT (新兴架构)
  * **本文主角:** ResMLP (S12, S24, B24 三种尺寸)
* **评估指标：**
  * **性能:** Top-1 准确率 (Acc.)
  * **效率:** 参数量 (#params), 吞吐量 (throughput), 计算量 (FLOPs), 峰值内存 (Peak Mem)

**核心结论：**

* `ResMLP` 是一种极具竞争力的新型架构，在性能上足以和强大的 Transformer 模型相抗衡。
* 该模型在**性能与效率之间取得了出色的权衡 (Trade-off)**。例如，`ResMLP-S12` 模型虽然准确率不是最高，但其极低的计算量和极高的吞吐量，使其非常适合需要高效率的场景。
* 实验证明，不依赖于卷积或自注意力机制的简单前馈网络，也能够实现顶尖的图像分类性能。

---

## 实验二：迁移学习能力评估 (Transfer Learning)

**目的：** 测试 `ResMLP` 模型作为预训练模型，其学习到的通用视觉特征迁移到其他不同领域的下游任务时的表现如何。

**方法：**

1. 所有模型首先在大型通用数据集 `ImageNet-1k` 上进行预训练。
2. 然后，将预训练好的模型在多个专业的下游数据集上进行微调 (Fine-tuning)。
3. 比较各模型在下游任务上的最终分类准确率。

* **下游数据集：** CIFAR-10, CIFAR-100, Flowers102, Cars, iNat18, iNat19 等。

**核心结论：**

* `ResMLP` 是一位**优秀的迁移学习者**。
* 尽管其计算成本（FLOPs）远低于同级别的 ViT 和 DeiT 模型，`ResMLP` 在多个迁移任务上的准确率与之非常接近，甚至在某些细粒度分类任务上表现更优。
* 这表明 `ResMLP` 学习到的特征具有很好的**泛化能力**，可以作为强大的特征提取器，有效地应用于各种不同的视觉任务。

---

## 实验三：消融实验 (Ablation Study)

**目的：** 通过系统性地移除或替换 `ResMLP` 模型中的关键组件，来理解每个部分对模型最终性能的实际贡献和重要性。

**方法：** 以一个性能优良的基准 `ResMLP` 模型为基础，逐一改变其设计，并观察性能变化。

**核心结论：**

* **证明了模型设计的合理性：**
  * **模块间通信 (Patch communication):** 当**完全移除**此模块后，模型准确率从 76.6% **暴跌至 56.5%**，证明了这是 `ResMLP` 成功的**绝对核心**。
  * **归一化 (Normalization) 与池化 (Pooling):** 将作者的设计替换为其他标准方案后，性能均有不同程度的下降，证明了作者选择的合理性。
* **量化了各组件的贡献度：**
  * **训练策略:** 使用更先进的训练方法（如知识蒸馏、自监督预训练）或更大的预训练数据集（ImageNet-21k）能**显著提升**模型性能。
  * **图像块大小 (Patch size):** 使用更小的图像块可以提升准确率，但会**急剧增加计算成本**。

> **总结：** 消融实验清晰地展示了 `ResMLP` 的每一个设计决策都是经过深思熟虑且行之有效的，极大地增强了该研究工作的可信度和说服力。

---

## 实验四：自监督学习与内部机理可视化

**目的：**

1. 评估 `ResMLP` 在不使用任何人工标签的情况下，从数据自身学习特征的能力。
2. 通过可视化模型内部权重，尝试“打开黑箱”，理解 `ResMLP` 的工作机理。

**方法：**

* **性能评估:** 使用 `DINO` 自监督学习方法训练 `ResMLP`，并与 `ResNet-50` 和 `ViT` 进行性能对比。
* **机理探索:** 将 `ResMLP` 不同深度线性层的权重矩阵进行可视化。

**核心结论：**

* **性能上：** `ResMLP` 能够有效地进行自监督学习，其性能虽不及为此优化的 ViT，但与经典的 ResNet-50 处于可比较的水平。
* **机理上（最惊人的发现）：**
  * 模型的可视化结果显示，**`ResMLP` 在训练过程中自发地学习出了类似卷积核的模式**。
  * 在浅层网络，它学会了类似 `1x1` 卷积的局部点关注。
  * 在中层网络，它学会了类似 `3x3` 或 `5x5` 卷积的十字星形模式，关注邻近区域。
  * 这证明了 `ResMLP` 尽管结构上没有卷积，但其功能上成功**复现了卷积网络强大的“局部归纳偏置”**，为模型的成功提供了直观且有力
